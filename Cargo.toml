[package]
name = "longqlora"
version = "3.0.0"
edition = "2021"
description = "Ultra-High Performance LLM Context Extension with Rust - 10x-100x faster than Python"
authors = ["AI Assistant <assistant@example.com>"]
license = "MIT OR Apache-2.0"
keywords = ["ml", "llm", "attention", "lora", "rust", "cuda", "performance"]
categories = ["science", "algorithms", "mathematics"]

[dependencies]
# Core ML and tensor operations
candle-core = { version = "0.6", features = ["cuda", "cudnn", "metal"] }
candle-nn = "0.6"
candle-transformers = "0.6"
candle-flash-attn = { version = "0.6", optional = true }

# High-performance computing
tokio = { version = "1.0", features = ["full"] }
rayon = "1.8"

# Serialization and config
serde = { version = "1.0", features = ["derive"] }
serde_yaml = "0.9"
serde_json = "1.0"

# CLI and logging
clap = { version = "4.0", features = ["derive"] }
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
indicatif = "0.17"

# Async and networking
reqwest = { version = "0.11", features = ["json", "stream"] }
futures = "0.3"

# Memory management
bytes = "1.0"
memmap2 = "0.9"

# Error handling
anyhow = "1.0"
thiserror = "1.0"

# Utilities
regex = "1.10"
rand = "0.8"
uuid = { version = "1.0", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }

[dependencies.tch]
version = "0.13"
optional = true

[features]
default = ["flash-attn"]
cuda = ["candle-core/cuda"]
flash-attn = ["candle-flash-attn"]
python-compat = ["tch"]
distributed = []
profiling = []

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
panic = "abort"
strip = true

[profile.dev]
opt-level = 2
debug = true

[[bin]]
name = "longqlora"
path = "src/main.rs"

[lib]
name = "longqlora"
path = "src/lib.rs"
